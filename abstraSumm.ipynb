{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:20:58.911243Z",
     "start_time": "2020-02-04T13:20:58.578088Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cDX7YrwIGK6M"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'hmm/mref'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-caee1d188996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hmm/mref'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'hmm/mref'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import spacy\n",
    "import datetime\n",
    "spacy.load('hmm/mref')\n",
    "import heapq\n",
    "import json\n",
    "import os.path\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from google.auth.transport.requests import Request\n",
    "# from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from nltk import ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy import displacy\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# from googleapiclient.discovery import build\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:31:38.530957Z",
     "start_time": "2020-02-04T12:31:38.526666Z"
    }
   },
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/calendar_entry']\n",
    "\n",
    "f = open('convotext.txt', 'r').read().lower()\n",
    "\n",
    "#     f = re.sub(r'\\s+', ' ', f)\n",
    "no_of_lines = len(open('convotext.txt', 'r').readlines())\n",
    "stop_words = set(\n",
    "    stopwords.words('english') +\n",
    "    ['i', 'he', 'me', 'she', 'it', 'them', 'her', 'him'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:32:05.725796Z",
     "start_time": "2020-02-04T12:32:05.722950Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punc(sent):\n",
    "    #\n",
    "    punctuations = '''!()-[]{};'\"\\,<>/?@#%^&*_~'''\n",
    "    for x in sent:\n",
    "        if x in punctuations:\n",
    "            sent = sent.replace(x, \" \")\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:32:05.770291Z",
     "start_time": "2020-02-04T12:32:05.726846Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    #\n",
    "    sent = remove_punc(sent)\n",
    "    sent = nltk.word_tokenize(sent, language='english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sent = [lemmatizer.lemmatize(x) for x in sent]\n",
    "    sent = ' '.join(sent)\n",
    "    filtered_sentence = [\n",
    "        w for w in sent.split(' ') if not w.lower() in stop_words\n",
    "    ]\n",
    "\n",
    "    return ' '.join(filtered_sentence).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:32:05.793961Z",
     "start_time": "2020-02-04T12:32:05.773798Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_freq(sent):\n",
    "    word_frequencies = {}\n",
    "    for word in sent:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word] / maximum_frequncy)\n",
    "\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:32:05.820166Z",
     "start_time": "2020-02-04T12:32:05.797012Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_score_calc(text, word_frequencies):\n",
    "    sentence_list = nltk.sent_tokenize(text)\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                # if len(sent.split(' ')) < 10:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:33:59.286164Z",
     "start_time": "2020-02-04T12:33:59.282929Z"
    }
   },
   "outputs": [],
   "source": [
    "def extractive_summary(f, docu):\n",
    "    max_freq = weighted_freq(docu)\n",
    "    sent_scores = sent_score_calc(f, max_freq)\n",
    "    no_of_lines = len(docu.split('.'))\n",
    "    summary_sentences = heapq.nlargest(int(no_of_lines / 3),\n",
    "                                       sent_scores,\n",
    "                                       key=sent_scores.get)\n",
    "    # summary_sentences =sorted(sent_scores, key=sent_scores.get, reverse=True)[:int(no_of_lines/2)]\n",
    "\n",
    "    summary = ' '.join(i.capitalize() for i in summary_sentences)\n",
    "    # print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:32:37.435674Z",
     "start_time": "2020-02-04T12:32:37.417728Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\" \".join(f.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:34:01.156875Z",
     "start_time": "2020-02-04T12:34:01.142888Z"
    }
   },
   "outputs": [],
   "source": [
    "extractive_summary(\" \".join(f.split(\"\\n\")), docu=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:34:08.149031Z",
     "start_time": "2020-02-04T12:34:08.133274Z"
    }
   },
   "outputs": [],
   "source": [
    "def trends(js):\n",
    "    js = js.split(' ')\n",
    "    lis_trend = []\n",
    "    for each in js:\n",
    "        each = preprocess(each)\n",
    "        lis_trend.extend(each.split(' '))\n",
    "\n",
    "    dict_trend = Counter(lis_trend)\n",
    "    dict_trend['.'] = 0\n",
    "\n",
    "    wordcloud = WordCloud(width=500,\n",
    "                          height=500,\n",
    "                          background_color='white',\n",
    "                          stopwords=stop_words,\n",
    "                          min_font_size=7).generate(' '.join(\n",
    "                              list(set(lis_trend))))\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.show()\n",
    "    plt.axis('off')\n",
    "    plt.savefig('trends.png')\n",
    "\n",
    "    return dict_trend.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T12:35:03.072240Z",
     "start_time": "2020-02-04T12:35:01.245090Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trends(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:16:51.404240Z",
     "start_time": "2020-02-04T13:16:51.396530Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_train_gen():\n",
    "    # l = ['my no is 9003401119','is your phone 9341234441','your phone no is 8341934568','here is my no 8261348649','here is my no 6713401897']\n",
    "    l = [\n",
    "        \"my email is ~msubhaditya@gmail.com\",\n",
    "        \"is your email id ~rules@yahoo.com\",\n",
    "        \"your email is ~aditya@rediff.com\", \"here is email ~bce@mail.com\",\n",
    "        \"here is email id ~hello@find.in\"\n",
    "    ]\n",
    "    s = ''\n",
    "    for a in l:\n",
    "        s += \"[({},{},'EMAIL')]\\n\".format(\n",
    "            re.search(r'~', a).start() + 1, len(a))\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:16:52.422001Z",
     "start_time": "2020-02-04T13:16:52.417283Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_sp_model():\n",
    "    TRAIN_DATA = [(u\"my no is 9003401119\", {\n",
    "        \"entities\": [(9, 19, \"PHONE\")]\n",
    "    }), (u\"is your phone 9341234441\", {\n",
    "        \"entities\": [(14, 24, \"PHONE\")]\n",
    "    }), (u\"your phone number is 8341934568\", {\n",
    "        \"entities\": [(17, 27, \"PHONE\")]\n",
    "    }), (u\"here is my no 8261348649\", {\n",
    "        \"entities\": [(14, 24, \"PHONE\")]\n",
    "    }), (u\"here is my number 6713401897\", {\n",
    "        \"entities\": [(14, 24, \"PHONE\")]\n",
    "    }),\n",
    "        (u\"my email is msubhaditya@gmail.com\", {\n",
    "            \"entities\": [(12, 34, \"EMAIL\")]\n",
    "        }),\n",
    "        (u\"is your email id rules@yahoo.com\", {\n",
    "            \"entities\": [(17, 33, \"EMAIL\")]\n",
    "        }),\n",
    "        (u\"your email is aditya@rediff.com\", {\n",
    "            \"entities\": [(14, 32, \"EMAIL\")]\n",
    "        }),\n",
    "        (u\"here is email bce@mail.com\", {\n",
    "            \"entities\": [(14, 27, \"EMAIL\")]\n",
    "        }),\n",
    "        (u\"here my email id hello@find.in\", {\n",
    "            \"entities\": [(17, 31, \"EMAIL\")]\n",
    "        })]\n",
    "    nlp = spacy.blank('en')\n",
    "    # optimizer = nlp.begin_training()\n",
    "    # for i in range(20):\n",
    "    #     random.shuffle(TRAIN_DATA)\n",
    "    #     for text, annotations in TRAIN_DATA:\n",
    "    #         nlp.update([text], [annotations], sgd=optimizer)\n",
    "\n",
    "    batches = spacy.util.minibatch(TRAIN_DATA)\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations)\n",
    "    nlp.to_disk(\"newmod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:21:28.876042Z",
     "start_time": "2020-02-04T13:21:28.868765Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_context(docu):\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(docu)\n",
    "    fin_dic = {}\n",
    "    for ent in doc.ents:\n",
    "        fin_dic[ent.text] = ent.label_\n",
    "    return json.dumps(fin_dic, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:17:22.195505Z",
     "start_time": "2020-02-04T13:17:22.182628Z"
    }
   },
   "outputs": [],
   "source": [
    "def context_json(p):\n",
    "    dic = json.loads(return_context(p))\n",
    "    d_final = {'persons': [], 'phone': [], 'emails': [], 'date': []}\n",
    "    d_final['phone'].extend(re.findall(r'\\d{10}', p))\n",
    "    d_final['emails'].extend(re.findall(r'\\S+@\\S+', p))\n",
    "\n",
    "    for a in dic:\n",
    "        if dic[a] == 'PERSON':\n",
    "            d_final['persons'].append(a)\n",
    "        if dic[a] == 'DATE':\n",
    "            d_final['date'].append(a)\n",
    "    l = []\n",
    "    for a in d_final:\n",
    "        l.append(d_final[a])\n",
    "\n",
    "    pd.DataFrame({k: pd.Series(l) for k, l in d_final.items()}).to_csv('output.csv',columns = ['persons','phone','emails','date'])\n",
    "    # print(json.dumps(d_final))\n",
    "    return json.dumps(d_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T13:21:31.459162Z",
     "start_time": "2020-02-04T13:21:30.160542Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2c71cfa38f56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcontext_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "context_json(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "abstraSumm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
